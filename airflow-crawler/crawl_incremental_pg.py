import os
import datetime as dt
import time
import requests
from bs4 import BeautifulSoup
import psycopg2
from psycopg2.extras import execute_batch

BASE_API = "https://tinnhiemmang.vn/filterObj"
TOKEN = "K25yWvL6YCA4ZecPjFA5jgEWMvSrjoMFM4zVQmY5"   # Token t·ª´ Network
TODAY = dt.date.today()

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "*/*",
    "X-Requested-With": "XMLHttpRequest"
}

def fetch_page(page: int):
    """G·ªçi API filterObj"""
    params = {
        "_token": TOKEN,
        "name_obj": "",
        "type": "web",
        "page": page
    }
    r = requests.get(BASE_API, params=params, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return r.text


def parse_items(html_text):
    """Parse HTML t·ª´ API ƒë·ªÉ l·∫•y domain + c√¥ng ty"""
    soup = BeautifulSoup(html_text, "html.parser")
    items = []

    for li in soup.select("li"):
        # L·∫•y domain
        a = li.select_one("a")
        if not a:
            continue

        domain = a.get_text(strip=True)

        # L·∫•y c√¥ng ty
        info_block = li.get_text(" ", strip=True)
        company = ""
        if "S·ªü h·ªØu b·ªüi:" in info_block:
            company = info_block.split("S·ªü h·ªØu b·ªüi:")[1].strip()

        items.append((domain, company))
    return items


def crawl_all():
    """Crawl t·∫•t c·∫£ page ƒë·∫øn khi h·∫øt d·ªØ li·ªáu"""
    page = 1
    result = []

    while True:
        print(f"üîé Crawl page {page} ...")
        html = fetch_page(page)
        items = parse_items(html)

        if not items:
            print("‚õî H·∫øt d·ªØ li·ªáu, d·ª´ng crawl.")
            break

        print(f"   ‚Üí {len(items)} domain")
        result.extend(items)
        page += 1
        time.sleep(0.5)

    # dedup
    dedup = {}
    for d, c in result:
        dedup[d] = c or ""

    return [(d, dedup[d]) for d in dedup]


def upsert_rows(rows):
    if not rows:
        return 0

    PG_DSN = os.getenv("PG_DSN")
    conn = psycopg2.connect(PG_DSN)
    conn.autocommit = True

    with conn.cursor() as cur:
        # Ghi v√†o b·∫£ng white_listurl (ƒë√£ ƒë∆∞·ª£c t·∫°o b·ªüi migration)
        # S·ª≠ d·ª•ng domain l√†m unique constraint, n·∫øu domain ƒë√£ t·ªìn t·∫°i th√¨ update
        sql = """
        INSERT INTO white_listurl(domain, company, first_seen, last_seen, source)
        VALUES (%s, NULLIF(%s,''), %s, %s, 'tinnhiemmang')
        ON CONFLICT (domain)
        DO UPDATE SET
            company = COALESCE(EXCLUDED.company, white_listurl.company),
            last_seen = EXCLUDED.last_seen,
            source = COALESCE(white_listurl.source, 'tinnhiemmang');
        """

        execute_batch(cur, sql, [(d, c, TODAY, TODAY) for d, c in rows], 500)

    conn.close()
    return len(rows)


if __name__ == "__main__":
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl t·∫°i {TODAY}")
    rows = crawl_all()
    print(f"üìä T·ªïng: {len(rows)} domain sau dedup")

    n = upsert_rows(rows)
    print(f"‚úÖ ƒê√£ upsert {n} domain v√†o DB")
