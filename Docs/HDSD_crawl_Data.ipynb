{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 1: Chu·∫©n b·ªã m√¥i tr∆∞·ªùng (ch·∫°y √¥ n√†y tr∆∞·ªõc) => ch·∫°y tr√™n powershell"
      ],
      "metadata": {
        "id": "DMWnGDNqesch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√†i Docker v√† Docker Compose (ch·ªâ c·∫ßn l√†m 1 l·∫ßn tr√™n m√°y)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y docker.io docker-compose\n",
        "\n",
        "# Kh·ªüi ƒë·ªông Docker service\n",
        "!service docker start\n"
      ],
      "metadata": {
        "id": "vuQX3eY-fHpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 2: T·∫°o c·∫•u tr√∫c project tr√™n VSCODE"
      ],
      "metadata": {
        "id": "m88D6TkQfMMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T·∫°o th∆∞ m·ª•c d·ª± √°n\n",
        "!mkdir -p /content/crawl_project/dags\n",
        "%cd /content/crawl_project"
      ],
      "metadata": {
        "id": "x0i5x4XnetZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 3: T·∫°o file docker-compose.yml (VSCODE)"
      ],
      "metadata": {
        "id": "6NzB89fsfOat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compose_content = \"\"\"version: '3'\n",
        "services:\n",
        "  postgres:\n",
        "    image: postgres:15\n",
        "    container_name: crawl_project-postgres-1\n",
        "    environment:\n",
        "      POSTGRES_USER: airflow\n",
        "      POSTGRES_PASSWORD: airflow\n",
        "      POSTGRES_DB: airflow\n",
        "    ports:\n",
        "      - \"5435:5432\"\n",
        "    volumes:\n",
        "      - pgdata:/var/lib/postgresql/data\n",
        "    healthcheck:\n",
        "      test: [\"CMD-SHELL\", \"pg_isready -U airflow -d airflow\"]\n",
        "      interval: 5s\n",
        "      timeout: 5s\n",
        "      retries: 10\n",
        "\n",
        "  airflow:\n",
        "    image: apache/airflow:2.10.2\n",
        "    container_name: crawl_project-airflow-1\n",
        "    depends_on:\n",
        "      postgres:\n",
        "        condition: service_healthy\n",
        "    environment:\n",
        "      AIRFLOW__CORE__LOAD_EXAMPLES: \"False\"\n",
        "      AIRFLOW__CORE__EXECUTOR: \"LocalExecutor\"\n",
        "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: \"postgresql+psycopg2://airflow:airflow@postgres:5432/airflow\"\n",
        "      MAX_PAGE: \"2000\"\n",
        "      SLEEP: \"0.5\"\n",
        "    volumes:\n",
        "      - ./dags:/opt/airflow/dags\n",
        "      - ./crawl_incremental_pg.py:/opt/airflow/crawl_incremental_pg.py\n",
        "    command: bash -lc \"\n",
        "      pip install requests beautifulsoup4 psycopg2-binary &&\n",
        "      airflow db migrate &&\n",
        "      airflow users create -u admin -p admin -r Admin -e a@a -f a -l d || true &&\n",
        "      airflow webserver\n",
        "    \"\n",
        "    ports:\n",
        "      - \"8083:8080\"\n",
        "\n",
        "  adminer:\n",
        "    image: adminer\n",
        "    container_name: crawl_project-adminer-1\n",
        "    ports:\n",
        "      - \"8082:8080\"\n",
        "    depends_on:\n",
        "      - postgres\n",
        "\n",
        "volumes:\n",
        "  pgdata:\n",
        "\"\"\"\n",
        "with open(\"docker-compose.yml\", \"w\") as f:\n",
        "    f.write(compose_content)\n",
        "print(\"‚úÖ docker-compose.yml created.\")\n"
      ],
      "metadata": {
        "id": "Nj0TdcptfQDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 4: T·∫°o file crawl_incremental_pg.py (VSCODE)"
      ],
      "metadata": {
        "id": "kaMADFLwfR8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_code = \"\"\"\n",
        "import os, time, datetime as dt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_batch\n",
        "\n",
        "BASE_URL = \"https://tinnhiemmang.vn/danh-ba-ten-mien?title=&page=\"\n",
        "TODAY = dt.date.today()\n",
        "SLEEP = float(os.getenv(\"SLEEP\", \"0.5\"))\n",
        "EMPTY_STOP = 2\n",
        "MAX_PAGE = int(os.getenv(\"MAX_PAGE\", \"2000\"))\n",
        "PG_DSN = os.getenv(\"PG_DSN\", \"postgresql://airflow:airflow@postgres:5432/airflow\")\n",
        "\n",
        "def get_soup(url):\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def parse_list_items(soup):\n",
        "    rows = []\n",
        "    for row in soup.select(\".views-row\"):\n",
        "        domain_tag = row.select_one(\"a\")\n",
        "        if not domain_tag: continue\n",
        "        domain = (domain_tag.get_text(strip=True) or \"\").lower()\n",
        "        company = \"\"\n",
        "        info = row.get_text(\" \", strip=True)\n",
        "        if \"S·ªü h·ªØu b·ªüi:\" in info:\n",
        "            try:\n",
        "                company = info.split(\"S·ªü h·ªØu b·ªüi:\")[1].split(\"\\\\n\")[0].strip()\n",
        "            except: pass\n",
        "        if domain: rows.append((domain, company))\n",
        "    return rows\n",
        "\n",
        "def crawl_once():\n",
        "    page = 0\n",
        "    empty = 0\n",
        "    out = []\n",
        "    while page <= MAX_PAGE:\n",
        "        soup = get_soup(BASE_URL + str(page))\n",
        "        items = parse_list_items(soup)\n",
        "        if not items:\n",
        "            empty += 1\n",
        "            if empty >= EMPTY_STOP: break\n",
        "        else:\n",
        "            empty = 0\n",
        "            out.extend(items)\n",
        "        page += 1\n",
        "        time.sleep(SLEEP)\n",
        "    dedup = {}\n",
        "    for d, c in out: dedup.setdefault(d, c or \"\")\n",
        "    return [(d, dedup[d]) for d in dedup]\n",
        "\n",
        "def upsert_rows(rows):\n",
        "    if not rows: return 0\n",
        "    conn = psycopg2.connect(PG_DSN)\n",
        "    conn.autocommit = True\n",
        "    with conn, conn.cursor() as cur:\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS tinnhiemmang(\n",
        "            domain TEXT PRIMARY KEY,\n",
        "            company TEXT,\n",
        "            first_seen DATE NOT NULL,\n",
        "            last_seen DATE NOT NULL\n",
        "        );\n",
        "        \"\"\")\n",
        "        sql = \"\"\"\n",
        "        INSERT INTO tinnhiemmang(domain, company, first_seen, last_seen)\n",
        "        VALUES (%s, NULLIF(%s,''), %s, %s)\n",
        "        ON CONFLICT (domain) DO UPDATE\n",
        "          SET last_seen = EXCLUDED.last_seen,\n",
        "              company = COALESCE(EXCLUDED.company, tinnhiemmang.company);\n",
        "        \"\"\"\n",
        "        execute_batch(cur, sql, [(d, c, TODAY, TODAY) for d, c in rows], page_size=500)\n",
        "    conn.close()\n",
        "    return len(rows)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rows = crawl_once()\n",
        "    n = upsert_rows(rows)\n",
        "    print(f\"UPSERT {n} domains at {TODAY}\")\n",
        "\"\"\"\n",
        "with open(\"crawl_incremental_pg.py\", \"w\") as f:\n",
        "    f.write(crawl_code)\n",
        "print(\"‚úÖ crawl_incremental_pg.py created.\")\n"
      ],
      "metadata": {
        "id": "UY2foer-fTbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 5: T·∫°o file dags/tinnhiemmang_incremental_dag.py (VSCODE)"
      ],
      "metadata": {
        "id": "-RYLugCTfVXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dag_code = \"\"\"\n",
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "\n",
        "default_args = {\n",
        "    \"owner\": \"you\",\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(minutes=10),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    dag_id=\"tinnhiemmang_incremental_daily\",\n",
        "    default_args=default_args,\n",
        "    start_date=datetime(2025, 1, 1),\n",
        "    schedule_interval=\"0 8 * * *\",\n",
        "    catchup=False,\n",
        "    tags=[\"crawl\", \"open-source\"],\n",
        ") as dag:\n",
        "\n",
        "    run_incremental = BashOperator(\n",
        "        task_id=\"run_incremental\",\n",
        "        bash_command=\"python /opt/airflow/crawl_incremental_pg.py\"\n",
        "    )\n",
        "\n",
        "    run_incremental\n",
        "\"\"\"\n",
        "with open(\"dags/tinnhiemmang_incremental_dag.py\", \"w\") as f:\n",
        "    f.write(dag_code)\n",
        "print(\"‚úÖ DAG file created.\")\n"
      ],
      "metadata": {
        "id": "ArhaP-oqfW4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 6: Kh·ªüi ƒë·ªông to√†n h·ªá th·ªëng"
      ],
      "metadata": {
        "id": "yE01L59BfYdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!docker compose up -d\n",
        "!docker compose ps"
      ],
      "metadata": {
        "id": "2hVU1zKbfaC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 7: M·ªü giao di·ªán ki·ªÉm tra"
      ],
      "metadata": {
        "id": "kUU1SR8Vfc9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Airflow UI: üëâ http://localhost:8083\n",
        "\n",
        "ƒêƒÉng nh·∫≠p: admin / admin\n",
        "‚Üí B·∫≠t DAG tinnhiemmang_incremental_daily ‚Üí Trigger DAG ‚Üí xem Logs.\n",
        "\n",
        "Adminer UI: üëâ http://localhost:8082\n",
        "\n",
        "System: PostgreSQL\n",
        "\n",
        "Server: postgres\n",
        "\n",
        "Username: airflow\n",
        "\n",
        "Password: airflow\n",
        "\n",
        "Database: airflow\n",
        "‚Üí Xem b·∫£ng tinnhiemmang."
      ],
      "metadata": {
        "id": "YwgopuyFfdMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü© B∆∞·ªõc 8: Xem log ho·∫∑c d·ª´ng d·ªãch v·ª• (powershell)"
      ],
      "metadata": {
        "id": "Xq42CQY0ffnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xem log Airflow\n",
        "!docker compose logs airflow --tail 100\n",
        "\n",
        "# D·ª´ng to√†n b·ªô h·ªá th·ªëng\n",
        "!docker compose down\n"
      ],
      "metadata": {
        "id": "8P9tf_sQfhWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî• K·∫øt qu·∫£:\n",
        "\n",
        "C√†o to√†n b·ªô domain t·ª´ tinnhiemmang.vn (~6000 d√≤ng).\n",
        "\n",
        "T·ª± ƒë·ªông ch·∫°y 8:00 s√°ng m·ªói ng√†y.\n",
        "\n",
        "Th√™m domain m·ªõi ho·∫∑c c·∫≠p nh·∫≠t last_seen cho domain c≈©.\n",
        "\n",
        "Xem d·ªØ li·ªáu qua Adminer ho·∫∑c tr·ª±c ti·∫øp t·ª´ Postgres."
      ],
      "metadata": {
        "id": "yb7WsTZifkJB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lo-PQbC4fkeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}